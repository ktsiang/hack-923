{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup of the environment**\n",
    "---\n",
    "## **Installation**\n",
    "First we must install the packages and set the necessary environment variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "724.51s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "731.05s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "737.60s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "743.94s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "750.41s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langchain\n",
    "%pip install --quiet langchain-community\n",
    "%pip install --quiet beautifulsoup4\n",
    "%pip install --quiet langchain-ollama\n",
    "%pip install --quiet chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basic steps**\n",
    "---\n",
    "Import the required libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read and parse the website data**\n",
    "\n",
    "To read the website data as a document, we will use the `WebBaseLoader` from LangChain. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Firefly_Aerospace\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We are going to use Python's `split()` function to extract the required portion of the text. \n",
    "- The extracted text should be converted back to LangChain's `Document` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text from the website data document\n",
    "text_content = docs[0].page_content\n",
    "\n",
    "# The text content between the substrings \"code, audio, image and video.\" to\n",
    "# \"Cloud TPU v5p\" is relevant for this tutorial. You can use Python's `split()`\n",
    "# to select the required content.\n",
    "text_content_1 = text_content.split(\"is an American private aerospace firm based in\",1)[1]\n",
    "final_text = text_content_1.split(\"Firefly headquarters and factory are located in\",1)[0]\n",
    "\n",
    "# Convert the text to LangChain's `Document` format\n",
    "docs =  [Document(page_content=final_text, metadata={\"source\": \"local\"})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initialize Ollama's embedding model**\n",
    "To create the embeddings from the website data, Ollama's embedding model will be used to suports creating text embeddings.\n",
    "\n",
    "To use this embeeding model, `OllamaEmbeddings` will be imported from LangChain. More information about the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"tinyllama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Store the data using Chroma**\n",
    "To create a Chroma vector database from the website data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear database\n",
    "shutil.rmtree('./chroma_db')\n",
    "# # Save to disk\n",
    "vectorstore = Chroma.from_documents(\n",
    "        documents=docs,                 # Data\n",
    "        embedding=embeddings,    # Embedding model\n",
    "        persist_directory=\"./chroma_db\" # Directory to save data\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create a retriever using Chroma**\n",
    "\n",
    "- Retriever will be created to retrieve website data embeddings from the newly created Chroma vector store. \n",
    "- This retriever can be later used to pass embeddings that provide more context to the LLM for answering users queries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",       # Directory of db\n",
    "                        embedding_function=embeddings   # Embedding model\n",
    "                   )\n",
    "# Get the Retriever interface for the store to use later.\n",
    "# When an unstructured query is given to a retriever it will return documents.\n",
    "# Read more about retrievers in the following link.\n",
    "# https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "#\n",
    "# Since only 1 document is stored in the Chroma vector store, search_kwargs `k`\n",
    "# is set to 1 to decrease the `k` value of chroma's similarity search from 4 to\n",
    "# 1. If you don't pass this value, you will get a warning.\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Check if the retriever is working by trying to fetch the relevant docs related\n",
    "# to the word 'MMLU' (Massive Multitask Language Understanding). If the length is greater than zero, it means that\n",
    "# the retriever is functioning well.\n",
    "print(len(retriever.get_relevant_documents(\"MMLU\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generator**\n",
    "- The Generator prompts the LLM for an aswer when the user asks a questions. \n",
    "- The retriever created from the Chroma vector database will be used to \n",
    "  provide more context to the user's query. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "# model could be updated\n",
    "llm = OllamaLLM(model=\"tinyllama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create prompt templates**\n",
    "LangChain's `PromptTemplate` will be used to generate prompts to the LLM answering questions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use eight sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a stuff documents chain**\n",
    "- LangChain provides `Chains` for chaining togerther LLM with each other or other components for complex applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from documents to readable string format.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create stuff documents chain using LCEL.\n",
    "#\n",
    "# This is called a chain because you are chaining together different elements\n",
    "# with the LLM. In the following example, to create the stuff chain, you will\n",
    "# combine the relevant context from the website data matching the question, the\n",
    "# LLM model, and the output parser together like a chain using LCEL.\n",
    "#\n",
    "# The chain implements the following pipeline:\n",
    "# 1. Extract the website data relevant to the question from the Chroma\n",
    "#    vector store and save it to the variable `context`.\n",
    "# 2. `RunnablePassthrough` option to provide `question` when invoking\n",
    "#    the chain.\n",
    "# 3. The `context` and `question` are then passed to the prompt where they\n",
    "#    are populated in the respective variables.\n",
    "# 4. This prompt is then passed to the LLM (`gemini-pro`).\n",
    "# 5. Output from the LLM is passed through an output parser\n",
    "#    to structure the model's response.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prompt the model**\n",
    "Now we can querry the LLM by passing any questions to the `invoke()` functions of the stuff documents chain that we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"Please give me a summary?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "We have successfully created an LLM application that answers questions using data from a website with the help of LLM, LangChain, and Chroma."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
